services:
  # ==================== ZOOKEEPER (Required for Kafka) ====================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - ecommerce-net

  # ==================== KAFKA (Data Streaming) ====================
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - ecommerce-net

  # ==================== MONGODB (Hot Storage) ====================
  mongodb:
    image: mongo:7.0
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db
    networks:
      - ecommerce-net

  # ==================== REDIS (Caching Layer) ====================
  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - ecommerce-net

  # ==================== HADOOP NAMENODE (Archive Storage) ====================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=ecommerce
    env_file:
      - ./config/hadoop.env
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
    networks:
      - ecommerce-net

  # ==================== HADOOP DATANODE ====================
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - namenode
    env_file:
      - ./config/hadoop.env
    volumes:
      - hadoop-datanode:/hadoop/dfs/data
    networks:
      - ecommerce-net

# ==================== SPARK MASTER ====================
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    hostname: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_NO_DAEMONIZE=true
      - PYSPARK_PYTHON=python3.9
      - PYSPARK_DRIVER_PYTHON=python3.9
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - ecommerce-net

  # ==================== SPARK WORKER ====================
  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - PYSPARK_PYTHON=python3.9
      - PYSPARK_DRIVER_PYTHON=python3.9
    networks:
      - ecommerce-net

# ==================== POSTGRES (Airflow DB) ====================
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - ecommerce-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==================== AIRFLOW INIT ====================
  airflow-init:
    build: ./airflow
    container_name: airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: /init-airflow.sh
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - ecommerce-net

  # ==================== AIRFLOW WEBSERVER ====================
  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "ecommerce_secret"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    ports:
      - "8081:8080"
    networks:
      - ecommerce-net
    command: webserver

  # ==================== AIRFLOW SCHEDULER ====================
  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "ecommerce_secret"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks:
      - ecommerce-net
    command: scheduler
  
  # ==================== DATA GENERATOR ====================
  data-generator:
    build:
      context: ./data_generator
      dockerfile: Dockerfile
    container_name: data-generator
    depends_on:
      - kafka
    environment:
      - KAFKA_BROKER=kafka:9092
    networks:
      - ecommerce-net
    restart: unless-stopped

  # ==================== DASHBOARD (Streamlit) ====================
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: dashboard
    depends_on:
      - mongodb
      - redis
    ports:
      - "8501:8501"
    environment:
      - MONGO_HOST=mongodb
      - REDIS_HOST=redis
    networks:
      - ecommerce-net

# ==================== KAFKA TO MONGO CONSUMER ====================
  kafka-consumer:
    build:
      context: ./scripts
      dockerfile: Dockerfile
    container_name: kafka-consumer
    depends_on:
      - kafka
      - mongodb
    environment:
      - KAFKA_BROKER=kafka:9092
    networks:
      - ecommerce-net
    restart: unless-stopped

# ==================== POSTGRES (Hive Metastore Backend) ====================
  postgres-hive:
    image: postgres:13
    container_name: postgres-hive
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_DB=metastore
    networks:
      - ecommerce-net
    volumes:
      - postgres_hive_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive"]
      interval: 10s
      timeout: 5s
      retries: 5

# ==================== HIVE METASTORE ====================
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    depends_on:
      postgres-hive:
        condition: service_healthy
      namenode:
        condition: service_started
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-hive:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive
    ports:
      - "9083:9083"
      - "10000:10000"
      - "10002:10002"
    networks:
      - ecommerce-net
    entrypoint:
      - /bin/bash
      - -c
      - |
        export HADOOP_CLIENT_OPTS="-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-hive:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive"
        
        echo "Waiting for PostgreSQL..."
        sleep 10
        
        echo "Initializing Hive schema..."
        if /opt/hive/bin/schematool -dbType postgres -info 2>&1 | grep -q "schemaTool completed\|Hive distribution version"; then
          echo "Schema exists"
        else
          /opt/hive/bin/schematool -dbType postgres -initSchema --verbose || exit 1
        fi
        
        echo "Starting Hive Metastore in background..."
        /opt/hive/bin/hive --service metastore &
        
        sleep 10
        
        echo "Starting HiveServer2..."
        /opt/hive/bin/hive --service hiveserver2
        
# ==================== VOLUMES ====================
volumes:
  mongo-data:
  postgres-data:
  hadoop-namenode:
  hadoop-datanode:
  redis-data:
  postgres_hive_data:

# ==================== NETWORK ====================
networks:
  ecommerce-net:
    driver: bridge