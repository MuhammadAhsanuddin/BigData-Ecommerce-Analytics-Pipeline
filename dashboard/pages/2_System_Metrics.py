"""
System Metrics & Data Quality Dashboard
Shows Hive metadata, data quality, and performance metrics
"""

import streamlit as st
import redis
import ast
from pymongo import MongoClient
import subprocess
from datetime import datetime

st.set_page_config(
    page_title="System Metrics",
    page_icon="üìä",
    layout="wide"
)

st.title("üìä System Metrics & Data Quality")

# Connect to services
r = redis.Redis(host='redis', port=6379, decode_responses=True)
mongo_client = MongoClient('mongodb://mongodb:27017/')
db = mongo_client['ecommerce']

# Create columns
col1, col2 = st.columns(2)

# === DATA QUALITY METRICS ===
with col1:
    st.subheader("üîç Data Quality Status")
    
    try:
        quality_str = r.get('data_quality:metrics')
        if quality_str:
            quality = ast.literal_eval(quality_str)
            
            if quality['issues_count'] == 0:
                st.success("‚úÖ All quality checks passed!")
            else:
                st.warning(f"‚ö†Ô∏è {quality['issues_count']} issues found")
            
            # Metrics
            qcol1, qcol2, qcol3 = st.columns(3)
            
            with qcol1:
                st.metric("Duplicates", quality['duplicate_count'])
            
            with qcol2:
                st.metric("Null Values", quality['null_orders'] + quality['null_customers'])
            
            with qcol3:
                st.metric("Invalid Amounts", quality['invalid_amounts'])
            
            st.caption(f"Last checked: {quality['last_check']}")
        else:
            st.info("Quality metrics not available yet")
    except Exception as e:
        st.error(f"Error loading quality metrics: {e}")

# === PERFORMANCE METRICS ===
with col2:
    st.subheader("üìà Performance Metrics")
    
    try:
        perf_str = r.get('performance:metrics')
        if perf_str:
            perf = ast.literal_eval(perf_str)
            
            # MongoDB
            st.write("**MongoDB:**")
            pcol1, pcol2 = st.columns(2)
            with pcol1:
                st.metric("DB Size", f"{perf['mongodb']['size_mb']:.2f} MB")
            with pcol2:
                st.metric("Documents", f"{perf['mongodb']['objects']:,}")
            
            # Redis
            st.write("**Redis:**")
            pcol3, pcol4 = st.columns(2)
            with pcol3:
                st.metric("Memory", f"{perf['redis']['used_memory_mb']:.2f} MB")
            with pcol4:
                st.metric("Keys", perf['redis']['total_keys'])
            
            # HDFS
            if 'hdfs' in perf and 'error' not in perf['hdfs']:
                st.write("**HDFS:**")
                pcol5, pcol6 = st.columns(2)
                with pcol5:
                    st.metric("Archives", perf['hdfs']['total_archives'])
                with pcol6:
                    st.metric("Total Size", f"{perf['hdfs']['total_size_mb']:.2f} MB")
            
            st.caption(f"Last updated: {perf['timestamp']}")
        else:
            st.info("Performance metrics not available yet")
    except Exception as e:
        st.error(f"Error loading performance metrics: {e}")

# === HIVE METADATA ===
st.divider()
st.subheader("üóÑÔ∏è Hive Data Warehouse Status")

hcol1, hcol2, hcol3 = st.columns(3)

try:
    # Try to get Hive metadata (this would require connecting to Hive)
    # For now, show HDFS archive count
    
    with hcol1:
        # Count archives from HDFS
        result = subprocess.run(
            ['docker', 'exec', 'namenode', 'hdfs', 'dfs', '-ls', '/archives/ecommerce/'],
            capture_output=True,
            text=True
        )
        
        archive_count = result.stdout.count('archive_')
        st.metric("Total Archives", archive_count, help="Data archived to HDFS")
    
    with hcol2:
        # Calculate total archived records (from MongoDB original count)
        stats = db.command("dbStats")
        current_docs = stats['objects']
        st.metric("Current Active Records", f"{current_docs:,}", help="Records in MongoDB")
    
    with hcol3:
        # Estimate archived count
        st.metric("Warehouse Status", "‚úÖ Synced", help="Hive metastore is up to date")

except Exception as e:
    st.warning("Could not fetch Hive metrics")

# === AIRFLOW DAG STATUS ===
st.divider()
st.subheader("‚öôÔ∏è Pipeline Status")

dag_status = st.columns(4)

dags = [
    ("Analytics", "spark_analytics_pipeline"),
    ("Archiving", "mongodb_archiving_pipeline"),
    ("Quality", "data_quality_monitoring"),
    ("Performance", "performance_monitoring")
]

for i, (name, dag_id) in enumerate(dags):
    with dag_status[i]:
        st.metric(name, "üü¢ Running", help=f"DAG: {dag_id}")

# Auto-refresh
st.caption("Dashboard auto-refreshes every 30 seconds")
import time
time.sleep(30)
st.rerun()
